1.sudo apt install openjdk-14-jdk –y

 



2.java –version

 
3.sudo addgroup hadoop
 

4.sudo adduser --ingroup hadoop hadoopuser

 

5.sudo adduser hadoopuser sudo

 



6.sudo apt-get install openssh-server

 
7.su – hadoopuser

 

8.ssh-keygen -t rsa 

 
9.cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys

 

10.chmod 640 ~/.ssh/authorized_keys
 
11.ssh localhost
     Enter yes when Prompt. 
12.cd /home/testing/Desktop/

 




13.sudo tar -xvzf hadoop-3.3.0.tar.gz

 
14.sudo mv hadoop-3.3.0 /usr/local/hadoop

 
15.sudo chown -R hadoopuser /usr/local/hadoop
 
16.ssh localhost
 

17.su – hadoopuser
 
18. nano ~/.bashrc
   add the following content at the end of file

export HADOOP_HOME=/usr/local/hadoop
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"

 

Press the combination ctrl+o=”to save the file”,then ctrl+x=”to exit from the file

19.source ~/.bashrc
 

20.which javac
 
21.readlink -f /usr/bin/javac

 

22.sudo nano $HADOOP_HOME/etc/hadoop/hadoop-env.sh
 
	
  Paste the following content on the location in the file shown      in image 

export JAVA_HOME=/usr/lib/jvm/java-14-openjdk-amd64 
export HADOOP_CLASSPATH+=" $HADOOP_HOME/lib/*.jar"
 
  
Press the combination ctrl+o=”to save the file”,then ctrl+x=”to   exit from the file

23.cd /usr/local/hadoop/lib

 




24.sudo wget https://jcenter.bintray.com/javax/activation/javax.activation-api/1.2.0/javax.activation-api-1.2.0.jar
 

25. hadoop version

 

26.sudo nano $HADOOP_HOME/etc/hadoop/core-site.xml
   Paste the following content on the location in the file shown      in image 
    <configuration>
       <property>
       <name>fs.default.name</name>
       <value>hdfs://0.0.0.0:9000</value>
       <description>The default file system URI</description>
       </property>
    </configuration>



 
  Press the combination ctrl+o=”to save the file”,then ctrl+x=”to   exit from the file


27. sudo mkdir -p /home/hadoop/hdfs/{namenode,datanode}

 

28.sudo chown -R hadoopuser:hadoop /home/hadoop/hdfs

 

29.sudo nano $HADOOP_HOME/etc/hadoop/hdfs-site.xml
   Paste the following content on the location in the file shown     in image
    <configuration>
       <property>
         <name>dfs.replication</name>
         <value>1</value>
       </property>
       <property>
         <name>dfs.name.dir</name>
         <value>file:///home/hadoop/hdfs/namenode</value>
       </property>
       <property>
         <name>dfs.data.dir</name>
         <value>file:///home/hadoop/hdfs/datanode</value>
       </property>
    </configuration>
  
  Press the combination ctrl+o=”to save the file”,then ctrl+x=”to   exit from the file
30.sudo nano $HADOOP_HOME/etc/hadoop/mapred-site.xml
  Paste the following content on the location in the file shown     in image
   <configuration>
      <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
      </property>
   </configuration>
 

  Press the combination ctrl+o=”to save the file”,then ctrl+x=”to   exit from the file
31.sudo nano $HADOOP_HOME/etc/hadoop/yarn-site.xml
  Paste the following content on the location in the file shown     in image
   <configuration>
     <property>
       <name>yarn.nodemanager.aux-services</name>
       <value>mapreduce_shuffle</value>
     </property>
   </configuration>

 
Press the combination ctrl+o=”to save the file”,then ctrl+x=”to   exit from the file

32.su - hadoopuser
 

33.hdfs namenode –format
 

 



34.start-dfs.sh

 

35.start-yarn.sh

 

36.jps
 


